\chapter{Introduction}

Today's technology provide unprecedented opportunities to scientists for deriving, expanding, or correcting scientific theories. In the past few decades, we  have seen an ever increasing capability of acquire, store, and process data. Simultaneously, the scientific visualization emerged as discipline, and started to play a crucial role in the pipeline of many scientists. In fact, visualization became the means through which scientists explore, evaluate, and present results. 
%
Propelled by a vibrant community, visualization techniques have become more widespread, and have successfully been applied in a variety of fields, including medical diagnosis, computational fluid dynamics, weather simulation, among others. The wide range of application, and the uniqueness of each field have further motivated the development of more complete and complex visualization technique, some of which include isosurface extraction, direct volume rendering, flow visualization, to name a few.  The amount of work published in each of these areas in the past 20 years is remarkable. Visualization researchers have pushed the boundaries and developed many advanced algorithms: some based on complex mathematical concepts, {\em e.g.} Morse-Smale complex; or require cutting-edge hardware, {\em e.g.}, GPU; or have hard practical constraints, {\em e.g.}, bound on triangle quality; huge input data; etc.

With the increasing complexity and importance of visualization techniques in the scientific pipeline, the reliability of visualization started to attract attention. In the past few years, there was an increase in the number of published articles related to the way humans perceive data, accuracy of visualizations, how to extract and depict uncertainty, how visualization compare to each other, and, more general, how \emph{reliable} are visualization.
%
By ``reliable'', we include topics of current interest of the visualization community that contributes to increase the expert's confidence in visualizations: uncertainty visualization; uncertainty quantification; evaluation; user perception; and verification.

In this context, the main contribution of this dissertation is to advance the theory and practice of \emph{verification of visualization algorithms and implementation}.  
%
We provide the theoretical background for the verification of visualization algorithms and the results of applying these techniques to several implementations. 
%
Before we dive into the details on how to verify visualization algorithms, we go over the motivations behind verifying visualizations. 
%
The motivation behind the need for rigorously testing algorithms and implementation comes from several sources: good practice in software engineer; decrease costs \cite{reports}; safety \cite{automobile,nuclear,stuff}; \emph{etc.}. These are the well-known (good) reasons for performing software verification. Here, instead, we  argue that  at the heart of the methodology for verifying visualization, one will find just good science, which, in our case, means  the scientific method. Because visualization has become such an important part in the scientific inquiry, some scientific endeavors, will require softwares to be throughly verified.
	
\section{The method of inquiry}

 New theories are put forward and evaluated through the scientific method: observations; hypothesis formulation; predictions and testing; and analysis of the results. Details on how to perform each of these steps vary according to the phenomena being studied. A particular important step in the process of deriving a valid scientific theory is the process of \emph{falsification}: the process by which the theory risky predictions are tested. From ``The Logic of Scientific Discovery''~\cite{popper-scientificdiscovery}:
 \begin{quote}
With the help of other statements, previously accepted, certain singular statements  --  which we may call ``predictions'' -- are deduced from the theory; especially predictions that are easily testable or applicable. From among these statements, those are selected which are not derivable from the current theory, and more especially those which the current theory contradicts. Next we seek a decision as regards these (and other) derived statements by comparing them with the results of practical applications and experiments. If this decision is positive, that is, if the singular conclusions turn out to be acceptable, or verified, then the theory has, for the time being, passed its test: we have found no reason to discard it. But if the decision is negative, or in other words, if the conclusions have been falsified, then their falsification also falsifies the theory from which they were logically deduced.
 \end{quote}
% 
An example of testing risky prediction is the classic Eddington's experiment of Einstein's theory of relativity~\cite{coles2001einstein}. Eddington conducted an experiment to measure the amount of light bending caused by the massive size of the Sun.  During the eclipse of 29 May 1919, Eddington photographed the Hyades star cluster and measured the amount light deflected. The accepted theory at the time, Newton's law of gravity, predicted some shift in the position of the stars. Einstein's theory of gravity predicted twice as much shift as Newton's theory. Because Einstein's prediction contradicted current theory, it can be considered a risky prediction.
The eclipse was photographed, and the deviations were measured.  In this context, two outcomes are possible: the expected (predicted) did not matched the observed deflection -- because no deflection is observed, or Newton's prediction is observed, or some other value is obtained --  in which case the theory would be refuted; or, the predicted deflection matched the observations, in which case, nothing can be said about the correctness of the theory, aside from the fact that it has not been proved wrong and has stood to risky tests. The more a theory is tested, the more trustworthy it becomes. 

The same idea of falsification can be applied to test the trustworthiness of a computer algorithm and implementation. During the course of scientific inquiry, scientists carefully perform each step in the scientific method in order to mitigate and control errors. For each step of the scientific method, there are multiple ways of account for these errors: by using sophisticated statistical methods; advanced mathematical models; high-precision equipments; redundancy; etc.  The reason behind this is that the reliability of the conclusions drawn depends on how each of the steps are performed. In the example of Eddington's experiment, a series of precautions had to be made and several different error sources were taken into account in order to show that Einstein's predictions were correct\footnote{They started by sending off two expeditions. The first to Sobral, northern Brazil; and another to the island of Pr\'incipe, northern of S\~ao Tom\'e and Pr\'incipe. Aside equipment related to the telescope, backup lenses were packed along with equipment necessary to account for the rotation of the Earth. The expedition at Pr\'incipe, among other problems, had to deal with clouds and rain and thus they were able to retrieve only two usable photos. The expedition at Sobral, on the other hand, had better weather but had problem due to rise of the temperature between the time that the telescope was assembled and the time of the eclipse. Parts of the telescope expanded and as a result the images were blurred. Another addition problem is related to the very small expected light deflection. Since photograph plates could expand and shrink with the temperature, deflection could be due to other factors other than deflection, such as shrinkage. Other source of errors are involved. According to Coles~\cite{coles2001einstein}, at the time the results were published, they were met with some skepticism. For more details, see Coles~\cite{coles2001einstein}.}. 
%
Since the scientific methodology is used to increase one's confidence in a particular statement, it can also be applied to the sub steps involved in the formulation of a scientific theory, which, in turn, builds up one's confidence in the theory itself. For instance, how can one reliably make decision based on the results of his/her simulation code? In this case, the ideas behind falsifying a pieces of code have been proved valuable. The need for proving that a determined piece of code is correct is of crucial importance for the scientific pipeline. The lack of such guarantees made the discipline of CFD -- which stands for Computational Fluid Dynamics -- be once referred to as Colorful Fluid Dynamics \cite{meroney2004wind}. The analog for the visualization literature is the  ``pretty picture'' \cite{Globus:1994:FWS:182452.182465}. Of course, both communities have developed frameworks and practices for dealing with these problem.

\section{Verification}

The meaning word ``verification'' vary according to the context in which it is inserted. When applied loosely, it refers to good coding practices
(such as use of versioning system), software testing (unit/regression tests), and even the process of debugging a code. These practices are obviously valuable for helping build a trustworthy software, but they are often \emph{ad hoc} and have limited scope. 
In this dissertation, verification of visualization algorithms and implementations is closely related to its use in Computer Science (\cs) and Computation Simulation (\cse). 
In \cs{} according to IEEE standards, 
\emph{verification} is the ``process of evaluating a 
system or component to determine whether the products of a given 
development phase satisfy the conditions imposed at the start of
that phase'' \cite{159342}.
%
In this context, the program \emph{specification} is the condition
imposed at the start phase and the verification process
ideally should guarantee that the resulting \emph{implementation}
({\em i.e.} final computer software) meets the 
specification exactly. 
%
In the past
few decades, several techniques have been developed
for attaining software verification, which include theorem
provers~\cite{Bowen95}, 
model checking~\cite{Clarke08},
fuzzing~\cite{bird83, godefroid08},  and others. This variety of
techniques is due to the difficult task of testing a program (either
a model or an implementation) which may contain millions of lines
of code and an exponentially large state-space where a
bug might be 
hidden. Because of this large number of paths,
verification can be considered as a
process where one accumulates evidence that a code is
correct~\cite{roach98}, rather than deriving a proof that the code is actually correct.
These techniques have been successfully applied not only for
verification of 
user-level computer code~\cite{1646374}, but also
hardware~\cite{seger92} and  
operating system kernel~\cite{1629596}. 

Verification techniques developed in \cse~community are focused in general on the
numerical solution of partial differential equations (PDEs) that 
models a physical phenomena of
interest. In this context, 
\emph{verification} is defined as the process of determining if a
\emph{computational model}, and its corresponding numerical solution, 
obtained by discretizing the \emph{mathematical model} (with corresponding
exact solution) of a physical event and the code
implementing the computational model  
can be used to represent the mathematical model of the event with 
sufficient accuracy~\cite{babuska04}.
%
This definition is closely related to the errors involved during
discretization and implementation. They are of great importance for
scientists because they can be used to assess which of the models,
mathematical or computational, should be refined.  A successful
approach for code verification, known to be sensitive to code
mistakes \cite{roach98}, is the order of accuracy test --- an
evaluation of the implementation behavior when submitted to successive
grid refinement \cite{roach98}. 

The verification tools proposed by the \cs{} and \cse{} communities covers
only part of the scientific pipeline. Given the importance of the visualization step
in the scientific pipeline,  the need for reliable visualizations slowly become central to the scientific community. This means that not only numerical software has to be verified, but also visualization algorithms and implementations.
%
Nevertheless, visualization has not fallen under the same
rigorous scrutiny as other components of the pipeline
like mathematical modeling and numerical simulation.
Unlike in traditional computational science and engineering areas,
there is no commonly accepted framework for verifying the accuracy, reliability, 
and robustness of visualization tools. This precludes
a precise quantification of the visualization error budget in the 
larger scientific pipeline.
Furthermore, very few investigations have focused on how the error originating from 
other components of the computational pipeline
impacts visualization algorithms. 

While the lack of a well-established framework for verifying visualization
tools has meant that a variety of analysis techniques have been
developed~\cite{zhou01,tory04}, we believe that visualization 
has achieved sufficient importance to warrant investigation of
stricter verification methodologies. Several authors have
already asserted this need~\cite{Globus:1994:FWS:182452.182465, globus95,kirby-vv-08}, and in this work we present techniques that are a concrete step towards reducing the gap between best practices in simulation science and visualization.

\section{Contributions}

We advocate that all visualization algorithms and implementation should be verified.
Hence, the main contributions to this dissertation is to advance the theory and practice of verifying visualizations. We do so by first extracting important mathematical properties of the algorithms under verification. Then, we verify if the implementation under verification honors that property by stress testing it. As in the case of falsification of scientific theories, a mismatch reveals a problem in the pipeline: the implementation may have a problem; or, perhaps, the algorithm is flawed; or the verification pipeline may have problems. In any case, a problem is revealed. On the other hand, in case the implementation honors the property of interest for all performed tests, then nothing can be said about the correctness of the implementation and algorithm, except that it has stood severe stress test. More specifically, our contribution can be summarized as follows:
\begin{enumerate}
\item Chapter \ref{chap:geometry}: verification of geometrical properties of isosurface extraction techniques.
\begin{enumerate}
\item We introduce a framework for verification of visualization tools based on the Method of Manufactured Solutions, and show how to apply it for the verification of geometrical properties of isosurface extraction algorithms and implementations;
\item We provide the required mathematical analysis for the implementation of the verification procedure. The important properties that should be honored are derived from this mathematical analysis;
\item We show concrete results, and show how this verification procedure helped us to find and fix problems in both algorithm and implementation. Moreover, we show that it is not trivial to find solutions that are both geometrically accurate and honors additional properties (such as triangle quality);
\item As a by-product of the verification, we detail the behavior of several freely available implementation;
\end{enumerate}
\item Chapter \ref{chap:topology}: verification of topological properties of isosurface extraction techniques.
\begin{enumerate}
\item We introduce a framework for verification of topological properties of isosurface extraction algorithms;
\item We derive a framework based on Digital Topology for the extraction of important invariants (topological properties) that should be honored by topologically correct implmentations;
\item In addition, we derive a framework based on Stratified Morse Theory for the extraction of important invariants;
\item We detail the behavior of several freely and commercially available implementations of isosurface extraction; We show that all but one implementation failed our tests;
\end{enumerate}
\item Chapter \ref{chap:mc33}: practical consideration on Marching Cubes 33 topological correctness.
\begin{enumerate}
\item We show that both the Marching Cubes 33 algorithm and implementation have problems that prevents its topological correctness. Moreover, one of the problems is traced back to its original publication;
\item We propose a new and alternative ways to deal with the issues raised;
\item Building on recent efforts on executable papers, we provide new ways to interact with our work so as to improving understanding and reproducibility of the results shown;
\end{enumerate}
\item Chapter \ref{chap:vr}: verification of volume rendering techniques.
\begin{enumerate}
\item We introduce a framework for verification of volume rendering algorithms and implementations;
\item We provide the error analysis of the standard volume rendering integral that is crucial for the verification procedure;
\item We show how we used this information to find and fix problems in widely used volume rendering  implementations. Moreover, we provide a first attempt to detect the sensitivity of the verification procedure;
\end{enumerate}
\item Chapter \ref{chap:aiaa}: flow visualization and reliable visualizations.
\begin{enumerate}
\item We provide an overview of the some of the topics involved in reliable visualization. We focus on flow visualization and supporting tools.
\end{enumerate}
\end{enumerate}
The goal of this dissertation is to provide another step towards creating a culture of verification inside the visualization and related communities.

